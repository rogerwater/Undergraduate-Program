# MAXQ Algorithm

用于分层强化学习的 MAXQ 方法的核心是 MAXQ 值函数分解。  MAXQ 描述了如何将策略的整体价值函数分解为各个子任务（以及子子任务，递归地）的价值函数集合。

## 1. Definitions

MAXQ 分解采用给定的 MDP M 并将其分解为一组子任务 $\{M_0, M_1, \cdots, M_n\}$ ，约定 $M_0$ 是根子任务（即求解 $M_0$ 就求解了整个原始 MDP $M$ ）。

### 1.1 Definition 1

一个未参数化的子任务是一个三元组 $<T_i, A_i, \tilde{R}_i>$ ，定义如下：

1. $T_i(s_i)$ 是一个终止谓词，它将 $S$ 划分为一组活动状态 $S_i$ 和一组终止状态 $T_i$。 仅当当前状态 $s$ 处于 $S_i$ 时，才能执行子任务 $M_i$ 的策略。
2. $A_i$ 是可以执行以实现子任务 $M_i$ 的一组动作。 这些动作可以是来自 $A$ 的原始动作（MDP 的原始动作集合），也可以是其他子任务，我们将用它们的索引 $i$ 表示。 我们将这些动作称为子任务 $i$ 的“子任务”。 如果子任务 $M_j$ 具有形式参数，则它可以在 $A_i$ 中出现多次，并且每次出现都必须指定将绑定到形式参数的实际值。 动作集 $A_i$ 可能因状态而异，因此从技术上讲，$A_i$ 是 $s$ 的函数。 然而，我们将在我们的符号中抑制这种依赖性。
3. $\tilde{R}_i(s'|s, a)$ 是伪奖励函数，它指定从状态 $s \in S_i$ 到最终状态 $s' \in T_i$ 的每次转换的伪奖励。 这个伪奖励告诉我们每个最终状态对该子任务的期望程度。 它通常用于为目标最终状态提供 0 的伪奖励，并为任何非目标最终状态提供负奖励。

$M$ 中的每个原始动作 $a$ 都是 MAXQ 分解中的原始子任务，使得 $a$ 始终可执行，它总是在执行后立即终止，并且其伪奖励函数统一为零。

### 1.2 Definition 2

分层策略 $\pi$ 是包含问题中每个子任务的策略的集合： $\pi = \{\pi_0, \cdots, \pi_n\}$ 。

在参数化任务中，策略也必须参数化，以便 $\pi$ 接受状态和形式参数的绑定，并返回选定的操作及其形式参数的绑定（如果有）。

![](.\1.png)

分层策略隐式定义了从当前状态 $s_t$ 和当前堆栈内容 $K_t$ 到原语动作 $a$ 的映射。执行该动作，并产生结果状态 $s_{t+1}$ 和结果堆栈内容 $K_{t+1}$。由于堆栈中添加了状态信息，因此分层策略相对于原始 MDP 是非马尔可夫的。

由于分层策略从状态 $s$ 和堆栈内容 $K$ 映射到操作，因此分层策略的值函数通常还必须为状态 $s$ 和堆栈内容 $K$ 的所有组合赋值。

### 1.3 Definition 3

分层价值函数，表示为 $V^{\pi}(<s, K>)$ ，给出了从状态 $s$ 开始、堆栈内容为 $K$ 的分层策略 $\pi$ 的预期累积奖励。

在本文中，我们主要只对分层策略的“顶层”值感兴趣，即堆栈 $K$ 为空时的值：$V^{\pi}(<s, nil>)$。 这是从状态 $s$ 开始并从层次结构的顶层开始执行层次结构策略的值。

### 1.4 Definition 4

The projected value function： $V^{\pi}(s)$ ，是从状态 $s$ 开始并从任务层次结构的根开始执行层次策略 $\pi$ 的值。

## 2. Decomposition of the Projected Value Function

分解基于如下定理：

给定任务 $M_0, \cdots, M_n$ 的任务图和分层策略 $\pi$ ，每个子任务 $M_i$ 定义一个状态为 $S_i$ ，动作为 $A_i$ ，概率转移函数为 $P^{\pi}_i(s',N|s,a)$ 和期望奖励函数为 $\bar{R}(s,a) = V^{\pi}(a,s)$ 的半马尔科夫决策过程，其中 $V^{\pi}(a,s)$ 是子任务 $M_a$ 在状态 $s$ 下的预测值函数。如果 $a$ 是原始动作，则 $V^{\pi}(a,s)$ 被定义为在 $s$ 中执行 $a$ 的预期立即奖励： $V^{\pi}(a,s) = \sum_{s'}P(s'|s,a)R(s'|s,a)$ 。

设 $Q_{\pi}(i, s, a)$ 为子任务 $M_i$ 在状态 $s$ 中执行动作 $a$ 然后遵循分层策略 $\pi$ 直到子任务 $M_i$ 终止的预期累积奖励。

完成函数： $C^{\pi}(i,s,a)$ ，在状态 $s$ 中调用子任务 $M_a$ 的子例程后完成子任务 $M_i$ 的预期折扣奖励。奖励会折回到 $a$ 开始执行的时间点。
$$
C^{\pi}(i,s,a) = \sum_{s',N}P^{\pi}_i(s',N|s,a) \gamma^N Q^{\pi}(i,s',\pi(s'))
$$
根据这个定义，我们可以将 Q 函数递归地表示为：
$$
Q^{\pi}(i,s,a) = V^{\pi}(a,s) + C^{\pi}(i,s,a)
$$
最后，我们可以将 $V^{\pi}(i,s)$ 的定义重新表达为
$$
V^{\pi}(i,s) = 
\begin{cases}
Q^{\pi}(i,s,\pi_i(s)) & \quad \text{if $i$ is composite} \\
\sum_{s'}P(s'|s,i) R(s'|s,i) & \quad \text{if $i$ is primitive} \\
\end{cases}
$$
MAXQ 分解如下图所示。

![](.\2.png)

一般来说， MAXQ 值函数具有以下形式。
$$
V^{\pi}(0, s) = V^{\pi}(a_m,s) + C^{\pi}(a_{m-1},s,a_m) + \cdots + C^{\pi}(a_1,s,a_2) + C^{\pi}(0,s,a_1)
$$

## 3. A Learning Algorithm for the MAXQ Decomposition

这种层次结构限制了可能的策略的空间，因此可能无法表示最优策略或其价值函数。

在 MAXQ 方法中，约束有两种形式。首先，在子任务内，仅允许一些可能的原始动作。其次，考虑具有子节点 $\{ M_{j1}, \cdots, M_{jk} \}$ 的 MAX 节点 $M_j$ ，$M_j$ 学习到的策略必须涉及执行这些子节点学习到的策略。

对策略施加这些约束的目的是结合先验知识，从而减少为找到良好策略而必须搜索的空间大小。 然而，这些限制可能使得学习最优策略变得不可能。如果我们无法学习最优策略，下一个最佳目标将是学习与给定层次结构一致（即可以由给定层次结构表示）的最佳策略。

通过 MAXQ 方法，我们将寻求一种更弱的最优形式：递归最优。

寻求递归最优性而不是分层最优性的原因是，递归最优性使得可以在不参考执行上下文的情况下解决每个子任务。 这种上下文无关的属性使共享和重用子任务变得更加容易。 这对于成功使用状态抽象也至关重要。

在实践中，我们采用以下简化方法来定义 $\tilde{R}$ 。对于每个子任务 $M_i$，我们定义两个谓词：终止谓词 $T_i$ 和目标谓词 $G_i$ 。 目标谓词定义了终止状态的一个子集，即“目标状态”，这些状态的伪奖励为 0。所有其他终止状态都有一个固定的常量伪奖励（例如，-100），其设置为使其始终为在目标状态下终止比在非目标状态下终止更好。 对于我们测试过 MAXQ 方法的问题，这种方法非常有效。

在 MAXQ 的实验中，我们发现 $T_i$ 和 $G_i$ 的定义很容易出错。 如果目标没有仔细定义，很容易创建一组导致无限循环的子任务。

### 3.1 MAXQ-0 Algorithm

MAXQ-0 算法仅适用于伪奖励函数 $\tilde{R}$ 始终为零的情况。

![](.\3.png)

上图为 MAXQ-0 算法的伪代码。MAXQ-0 是一个递归函数，从状态 $s$ 的 Max 节点 $i$ 开始执行当前的探索策略。 它执行操作直到到达最终状态，此时它返回已执行的原始操作总数的计数。 为了执行某个动作，MAXQ-0 递归地调用自身。 当递归调用返回时，它会更新节点 $i$ 的完成函数的值。 它使用原始动作的数量来适当地折扣结果状态 $s'$ 的值。 在叶子节点，MAXQ-0 更新估计的单步预期奖励 $V(i, s)$。 值 $\alpha_t(i)$ 是一个“学习率”参数，应在极限内逐渐减小到零。

为了使这个算法描述完整，必须指定两件事。 

首先，我们必须在第 12 行指定如何计算 $V_t(i, s')$，因为它不存储在 Max 节点中。 它是通过以下分解方程的修改版本计算的：
$$
\begin{aligned}
V_t(i,s) &= 
	\begin{cases}
		\max_a Q_t(i,s,a) & \text{if $i$ is composite} \\
		V_t(i,s) & \text{if $i$ is primitive} \\
	\end{cases}
	\\
Q_t(i,s,a) &= V_t(a,s) + C_t(i,s,a) \\
\end{aligned}
$$
为了使用这些方程计算 $V_t(i, s)$ ，我们必须对 MAXQ 图中从节点 $i$ 开始到叶子节点结束的所有路径进行完整搜索。 下图给出了实现深度优先搜索的递归函数 EvaluateMaxNode 的伪代码。 除了返回 $V_t(i, s)$ 之外，EvaluateMaxNode 还返回达到该值的叶子节点处的操作。  MAXQ-0 不需要该信息，但稍后当我们考虑学习的递归最优策略的非分层执行时，它会很有用。

![](.\4.png)

为了完成 MAXQ-0 的定义，必须指定的第二件事是探索策略 $\pi_x$ 。 我们要求 $\pi_x$ 是有序的 GLIE 策略。

算法 MAXQ-0 可以通过我们称为“所有状态更新”的技术进行扩展，以加速图的较高节点的学习。 当为状态 $s$ 中的 Max 节点 $i$ 选择一个动作 $a$ 时， $a$ 的执行将使环境经历一系列状态 $s = s_1, \cdots, s_N, s_{N+1} = s'$ 。 如果 $a$ 确实是 $s_1$ 中选择的最佳抽象动作，那么它也应该是状态 $s_2$ 到 $s_N$ 中（在节点 $i$ 处）选择的最佳动作。 因此，我们可以针对每个中间状态执行 MAXQ-0 中第 12 行的一个版本，如替换伪代码所示：

![](.\5.png)

### 3.2 MAXQ-Q Learning Algorithm

接下来让我们为任意伪奖励函数 $\tilde{R}_i(s)$ 设计一个学习算法。我们可以将伪奖励添加到 MAXQ-0 中，但这会改变 MDP $M$ 以获得不同的奖励函数。伪奖励“污染”了层次结构中计算的所有完成函数的值。由此产生的学习策略对于原始 MDP 来说不会是递归最优的。

这个问题可以通过学习两个补全函数来解决。 第一个 $C(i, s, a)$ 是我们迄今为止在本文中讨论的完成函数。 它计算在状态 $s$ 中执行操作 $a$ 然后遵循 $M_i$ 的学习策略后完成任务 $M_i$ 的预期奖励。 它是在不参考 $\tilde{R}_i$ 的情况下计算的。 父任务将使用此完成函数来计算 $V (i, s)$，即从状态 $s$ 开始执行操作 $i$ 的预期奖励。

第二个完成函数 $\tilde{C}(i, s, a)$ 是一个完成函数，我们将仅使用“内部”节点 $i$ 来发现任务 $M_i$ 的局部最优策略。 该函数将包含来自“真实”奖励函数 $R(s'|s, a)$ 和伪奖励函数 $\tilde{R}_i(s)$ 的奖励。

我们将采用两种不同的更新规则来学习这两个完成函数。 $\tilde{C}$ 函数将使用类似于 MAXQ-0 第 12 行 Q learning 规则的更新规则来学习。 但 $C$ 函数将使用类似于 SARSA(0) 的更新规则来学习，其目的是学习通过优化 $\tilde{C}$ 发现的策略的价值函数。

算法的伪代码 MAXQ-Q 如下所示。

![](.\6.png)

需要注意的是，该伪代码中凡是出现 $Vt(a, s)$ 的地方，都是指执行 Max 节点 $a$ 时状态 $s$ 的“未受污染”的值函数。 其递归计算方式与 MAXQ-0 中完全相同。

最后，请注意，伪代码还包含所有状态更新，因此每次调用 MAXQ-Q 都会返回执行期间访问过的所有状态的列表，并且对每个状态执行第 16 行和第 17 行的更新 。 状态列表按最近在先的顺序排列，因此状态会从上次访问的状态开始更新，并向后返回到起始状态，这有助于加快算法速度。

## 4. State Abstraction

1. 抽象层次的定义：对于每个层次（包括原始层次），定义抽象状态。这些抽象状态通常由底层状态的特征组成，可以捕获任务中的关键信息。
2. 状态抽象的映射：在每个层次上，定义状态抽象和原始状态之间的映射关系。这个映射关系指定了如何将底层状态映射到相应的抽象状态。这样，每个层次的任务都可以在抽象状态上进行学习和规划 。

通过状态抽象， MAXQ 能够在更高层次上组织任务结构，降低任务的复杂性，提高学习的效率。这样做的一个关键思想是，在不同层次上的学习过程相对独立，每个层次的学习都专注于特定的抽象表示，而不必处理过多的细节。

### 4.1 Five Conditions that Permit State Abstraction

#### 4.1.1 Condition 1: Max Node Irrelevance

当一组状态变量与 Max 节点无关时，就会出现第一个抽象条件。

此抽象条件成立的情况的一个规则是检查以给定 Max 节点 $i$ 为根的子图。如果一组状态变量与叶子节点的状态转移概率和奖励函数以及子图中的所有伪奖励函数和终止条件无关时，则这些变量满足 Max Node Irrelevance condition 。

#### 4.1.2 Condition 2: Leaf Irrelevance

第二个抽象条件描述了我们可以将状态抽象应用于 MAXQ 图的叶子节点的情况。

#### 4.1.3 Condition 3: Result Distribution Irrelevance

discount 会干扰引入基于 "funnel" 算子的状态抽象，因此 MAXQ 框架在 discount 设置下应用时效率较低。

"Funnel" 动作出现在许多分层强化学习问题中。只要我们处于无折扣设置下， Result Distribution Irrelevance 条件就适用于所有此类情况。

#### 4.1.4 Condition 4: Termination

当子任务保证导致其父任务在目标条件下终止时，条件 4 适用。从某种意义上说，子任务将环境汇集到父任务的目标谓词所描述的状态集中。

很容易检测到满足终止条件的情况。我们只需要将子任务的终止谓词与父任务的目标谓词进行比较，如果第一个包含第二个，则满足终止条件。

#### 4.1.5 Condition 5: Shielding

与 Termination 条件一样， Shielding 条件可以通过分析 MAXQ 图的结构并识别其祖先任务被终止的节点来验证。

### 4.2 Convergence of MAX-Q with State Abstraction

Conclusion:

1. 有序递归最优策略是一个抽象策略。
2. 当应用于具有安全状态抽象的 MAXQ 图时， MAXQ-Q 将收敛到该策略。

### 4.3 The Hierarchical Credit Assignment Problem

仍然存在一些情况，我们希望引入状态抽象，但上述五个属性不允许它们。

## 5. Non-Hierarchical Execution of the MAXQ Hierarchy

MDP 的最优策略通常不是严格的分层策略。

从层级政策的价值函数中推导出非层级政策。

第一种方法基于策略迭代的动态规划算法。策略迭代算法从初始策略 $\pi_0$ 开始。然后重复以下两个步骤，直到策略收敛。在策略评估步骤中，它计算当前策略 $\pi_k$的价值函数 $V^{\pi_k}$。然后，在策略改进步骤中，根据规则计算新策略 $\pi_{k+1}$ 。 
$$
\pi_{k+1}(s):= \underset{a}{\arg\max} \sum_{s'} P(s'|s,a) [R(s'|s,a) + \gamma V^{\pi_k}(s')]
$$
如果我们知道 $P(s'|s,a)$ 和 $R(s'|s,a)$ ，我们就可以使用价值函数的 MAXQ 表示来执行一步策略迭代。我们从分层策略 $\pi$ 开始，并使用 MAXQ 层次结构表示其价值函数。然后，我们可以通过使用 $V^{\pi}(0,s')$ 应用上述方程来计算 $V^{\pi}(s')$ ，从而执行策略改进的一步。

不幸的是，我们无法迭代这个策略改进过程，因为新策略不太可能是分层策略（即，它不太可能用 MAXQ 图的每个节点的局部策略来表示）。 尽管如此，政策的一步改进可以带来非常重大的改善。

这种非分层执行方法忽略了 MAXQ 图的内部结构。 实际上，MAXQ 层次结构只是被视为一种用于表示 $V^{\pi}$ 的函数逼近器——任何其他表示都会给出相同的一步改进策略 $\pi^g$ 。

第二种非分层执行方法借鉴了 Q-learning 的思想。价值函数 $Q$ 表示的一大优点是，我们可以在不知道 $P(s'|s,a)$ 的情况下计算策略改进的第一步，只需将新策略设为 $\pi^g(s) := \arg\max_a Q(s,a)$ 。这为我们提供了与上面使用一步前瞻计算相同的一步贪婪策略。通过 MAXQ 分解，我们可以在层次结构的各个级别执行这些策略改进步骤。

基于第二种方法的分层贪婪策略的伪代码如下所示。

![](.\7.png)

这可以提供比原始分层策略更好的策略。

总的来说，这两种非分层执行的方法都不占优势。 尽管如此，第一种方法仅在个体原始动作的层面上起作用，因此它无法对策略产生很大的改进。 相反，层次贪婪方法可以通过改变在层次结构的根附近选择哪些动作（即子例程）来获得策略上的非常大的改进。 因此，一般来说，分层贪婪执行可能是更好的方法。（当然，两种方法的价值函数都可以计算，并且可以执行具有更好估计值的方法。）

## 6. Design Tradeoffs in Hierarchical Reinforcement Learning

强调其中两个问题之间的权衡：**定义子任务的方法和状态抽象的使用**。

MAXQ 使用终止谓词 $T_i$ 和伪奖励函数 $\tilde{R}$ 定义子任务，这种方法至少有两个缺点。首先，很难正确定义 $T_i$ 和 $\tilde{R}$ ，因为这本质上需要猜测子任务终止的所有状态下 MDP 最优策略的值函数；其次，它引导我们寻求递归最优策略而不是层次最优策略，递归最优策略可能比层次最优策略差得多，因此我们可能会放弃大量性能。

然而，由于这两个缺点，MAXQ 获得了一个非常重要的好处：子任务的策略和值函数变得与上下文无关。换句话说，它们不依赖于其父任务或调用它们的更大上下文。终止谓词和伪奖励函数提供了一个屏障，阻止退出子任务与其上下文之间的值信息“通信”。

因此我们可以看到，实现层次最优性和实现递归最优性之间存在直接的权衡。层次优化方法在定义子任务方面有更大的自由度，但它们无法在子任务中使用状态抽象，而且通常它们无法在多个上下文中重用一个子任务的解决方案。另一方面，递归最优性的方法必须使用某种将子任务与其上下文隔离的方法来定义子任务，但其可以应用状态抽象，并且学习到的策略可以在许多上下文中重用。

此外， MAXQ-Q learning 的扩展可以在线调整 $\tilde{R}$ 值。每次子任务终止时，我们都可以根据终止状态的计算值更新 $\tilde{R}$ 函数。准确地说，如果 $j$ 是 $i$ 的子任务，那么当 $j$ 在状态 $s'$ 终止时，我们应该将 $\tilde{R}(j,s')$ 更新为 $\tilde{V}(i,s') = \max_{a'}\tilde{Q}(i,s',a')$ 。然而只用当使用完整状态 $s'$ 表示 $\tilde{R}(j,s')$ 时才有效。如果子任务 $j$ 采用状态抽象， $x = \mathcal{X}(s)$ ，则 $\tilde{R}(j,s')$ 将需要是 $\tilde{V}(i,s')$ 的平均值，其中平均值取所有使得 $x = \mathcal{X}(s)$ 的状态 $s'$ 。这可以通过每次子任务 $j$ 终止时执行以下形式的随机近似更新实现。这样的算法有望收敛到与给定状态抽象一致的最佳分层策略。
$$
\tilde{R}(j,x') = (1-\alpha_t)\tilde{R}(j,x') + \alpha_t \tilde{V}(i,s')
$$
在某些问题中，首先使用非常激进的状态抽象来学习递归最优策略，然后使用学习到的值函数来初始化具有更详细的状态表示的 MAXQ 表示可能是值得的。状态空间的这些渐进细化可以通过监控单个抽象状态 $x'$ 的 $\tilde{V} (i, s')$ 值变化程度来指导。如果它们有很大的差异，这意味着状态抽象未能对状态值做出重要区分，并且应该对其进行细化。

这两种自适应算法的收敛时间都比基本 MAXQ 方法要长，但对于智能体在其生命周期中必须多次解决的任务，值得拥有能够提供初始有用解决方案并逐渐改进该解决方案直至最佳的学习算法。

## 7. Concluding Remarks

MAXQ 分解可以表示在有限范围无折扣累积奖励准则和无限范围折扣奖励准则下任何分层策略的价值函数，这种表示支持子任务共享和重用，因为整体价值函数被分解为各个子任务的价值函数。

MAXQ-Q 学习以概率 1 收敛到递归最优策略。尽管递归最优性比分层最优性或全局最优性都要弱，但它是最优性的一种重要形式，因为它允许每个子任务学习局部最优策略，同时忽略 MAXQ 图中其祖先的行为。这增加了子任务共享和状态抽象的机会。

本文提出了两种不同的方法，用于从 MAXQ 值函数表示中导出改进的非分层策略，并形式化了这些方法可以改进分层策略的条件。

最后，本文认为分层强化学习方法的设计需要权衡。设计范围的一端是“上下文无关”方法，它们为状态抽象和子任务共享提供了良好的支持，但它们只能学习递归最优策略；另一方面是“上下文相关”方法，这些方法可以发现分层最优策略（或者在某些情况下，全局最优策略），但它们的缺点是它们不能轻松地利用状态抽象或共享子任务。由于状态抽象可以带来巨大的加速，本文认为上下文无关方法是首选，并且可以根据需要放宽它以获得改进的策略。

