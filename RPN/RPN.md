# Regression Planning Networks

## 1. Abstract

近期的 learning-to-plan 方法在直接从观测空间规划方面显示出有希望的结果，然而，他们规划长期任务的能力受到预测模型准确性的限制。另一方面，经典的符号规划器在解决长期任务方面表现出非凡的能力，但它们需要预定义的符号规则和符号状态，限制了它们在现实世界中的适用性。

在这项工作中，我们结合了这两种范式的好处，并提出了一种 learning-to-plan 的方法，该方法可以直接生成以高维观察为条件的长期符号计划。我们从经典规划文献中借用了回归（后向）规划的思想，并引入了回归规划网络（Regression Planning Networks, RPN），这是一种神经网络架构，它从任务目标开始向后规划，并生成一系列达到当前观察的中间目标。

我们表明，我们的模型不仅继承了符号规划的许多有利特征，例如解决以前看不见的任务的能力，而且还可以以端到端的方式从视觉输入中学习。

## 2. Introduction

经典的符号规划方法通常抽象出与真实符号的感知，并依靠预定义的规划域来指定动作的因果影响。这些假设极大地限制了这些方法在真实环境中的适用性，其中状态是高维的（例如彩色图像），并且指定详细的规划域是乏味的（如果不是不可能的话）。

**不依赖预定义动作模型和符号的规划解决方案是从观察中学会规划（plan from observations）。**

最近的研究表明，深度网络可以直接在观察空间或学习的潜在空间中捕获环境动态。通过学习到的动力学模型，这些方法可以通过前向预测将一系列动作规划到所需的目标。这种策略对于简单的短视距任务可能就足够了，例如，将对象推向某个位置，但它们在涉及较长时间尺度（例如，做饭）的高级决策的任务上表现不佳。

在这项工作中，我们旨在结合从观察中规划的优点和经典规划器的高级推理能力和可解释性。我们提出了一种 learning-to-plan 的方法，该方法可以从高维观察输入生成指向符号任务目标的长期计划。

我们建议在当前观察的符号空间条件下向后规划。我们的关键见解是，通过对当前观察进行调节，我们可以训练一个规划器直接预测搜索空间中将最终目标与当前观察连接起来的单个路径。由此产生的计划是一系列中间目标，可用于指导低级控制器与环境交互并实现最终任务目标。

我们提出了回归规划网络 (RPN)，这是一种神经网络架构，它学习在以环境观察为条件的符号规划空间中执行回归规划（后向规划）。架构的核心是预处理网络，它将当前观察和符号目标作为输入，并以相反的顺序迭代地预测一系列中间目标。此外，该架构通过使用依赖网络对符号子目标之间的依赖关系进行建模来利用符号空间的组合结构。这种依赖信息可用于将复杂的任务目标分解为更简单的子目标，这是学习复杂计划并泛化到新任务目标的基本机制。最后，我们提出了一种结合这些网络执行回归规划并调用低级控制器来执行环境中的规划的算法。

![](.\1.png)



我们的方法的一个优点是经过训练的 RPN 模型可以组成所见计划来解决训练数据集之外的新任务。

## 3. Related Work

有大量先前的工作来学习从观察中规划。然而，学习通过高维观察做出准确的预测仍然具有挑战性，尤其是对于长期任务。

最近的工作建议学习结构化的潜在表示进行规划。然而，学习这种表示依赖于重建完整的输入空间，这很难扩展到具有挑战性的视觉领域。

相反，我们的方法直接在符号空间中规划，这允许更有效的长期规划和可解释性，同时仍然以高维观察作为输入。

我们的方法不需要显式动作空间，直接从高级符号目标信息中学习，这使得它与代理无关和自适应到不同的低级控制器。

我们的方法不执行显式符号接地，可以直接从高维观察和任务目标生成计划。

给定一个目标和环境观察，我们的 RPN 预测一组在达到目标之前需要完成的前身目标。然后，回归规划过程是通过将预测目标反馈给网络来递归地应用 RPN，直到预测目标可以从当前观察到达。

## 4. Problem Definition and Preliminaries

### 4.1 Zero-shot Task Generalization with a Hierarchical Policy

零镜头任务泛化的目标是实现训练期间看不到的任务目标。

每个任务目标 $g$ 属于一组有效目标 $\mathcal{G}$ 。考虑转移概率为 $\mathcal{O} \times \mathcal{A} \times \mathcal{O} \to \mathbb{R}$ 的环境，其中 $\mathcal{O}$ 是一组环境观察， $\mathcal{A}$ 是一组原始动作。给定一个符号任务目标 $g$ ，智能体的目标是到达 $o \in \mathcal{O}_g$ ，其中 $\mathcal{O}_g \subset \mathcal{O}$ 是满足 $g$ 的观察集。我们采用分层策略设置，给定最终目标 $g$ 和当前观察 $o_t$ ，高级策略 $\mu: \mathcal{O} \times \mathcal{G} \to \mathcal{G}$ 生成中间目标 $g' \in \mathcal{G}$ ，低级策略 $\pi: \mathcal{O} \times \mathcal{G} \to \mathcal{A}$ 作用与环境中以实现中间目标。我们假设低级策略只能执行短期任务。在这项工作中，我们专注于学习有效的高级策略 $\mu$ ，并假设低级策略可以是预训练的智能体或规划器。

### 4.2 Regression Planning

在这项工作中，我们将高级策略 $\mu$ 制定为基于学习的回归规划器。

目标回归规划是一类符号规划算法，规划过程从目标向后运行，而不是向前到达目标。给定一个初始符号状态、符号目标和将动作定义为其前置条件和后置效应（即动作运算符）的规划域，回归规划器首先考虑可能导致目标的所有动作运算符，然后通过枚举动作运算符的所有前提条件来扩展搜索空间。该过程重复，直到当前符号状态满足运算符的前提条件。然后计划是一系列动作运算符，这会导致目标的状态。

因为我们不假设可以访问这些高级操作操作符（来自符号规划域）和当前符号状态，所以我们不能显式地执行这样一个详尽的搜索过程。相反，以当前环境观察为条件，我们的模型学习预测需要满足的先决条件，以实现目标。这种能力使我们的方法能够在没有显式动作运算符、规划域定义或符号状态的情况下执行回归规划。

我们将目标 $g \in \mathcal{G}$ 定义为一组逻辑原子的组合，每个原子由一个谓词和一个对象参数列表组成。我们用 $g$ 表示子目标 $g_i$ 中每个原子。子目标也可以被视为单个原子的目标。我们将目标 $g$ 或子目标 $g_i$ 的前提条件定义为另一个中间目标 $g' \in \mathcal{G}$ ，在根据环境观察尝试 $g$ 或 $g_i$ 时前需要满足。

## 5. Method

我们的主要贡献是引入回归规划的学习公式，并提出回归规划网络（RPN）作为解决方案。在这里，我们总结了作为学习问题提出的基本回归规划步骤，介绍了我们的模型使用的状态表示，并解释了我们解决回归规划的学习方法。

**Subgoal Serialization** ：子目标序列化思想源于偏序规划，其中规划目标被分为子目标，并且可以组合每个子目标的计划以降低搜索复杂度。挑战在于按顺序执行子目标计划，以便计划不会撤销已经实现的子目标。找到此类排序的过程称为子目标序列化。我们的方法明确地对子目标之间的依赖关系进行建模，并将子目标序列化制定为有向图预测问题。这是我们学习实现复杂目标并泛化到新目标的方法的重要组成部分。

**Precondition Prediction** ：在试图实现另一个目标之前找到需要满足的前身目标是向后规划的关键步骤。符号回归规划器依赖于规划域中定义的一组高级动作运算符来枚举有效的前提条件。这里的挑战是直接预测给定环境观察的目标的先决条件，而不假设个规划域。我们将预测前提条件的问题制定为图节点分类问题。

整体回归规划过程如下：给定一个任务目标，我们（1）将最终任务目标分解为子目标并找到完成子目标的最佳排序（子目标序列化），（2）预测每个子目标的前提条件，（3）将前提条件设置为最终目标，递归重复（1）和（2）。我们使用递归神经网络架构和调用网络执行回归规划的算法来实现该过程。

**Object-Centric Representation** ：为了在目标的符号表示和原始观察之间架起桥梁，我们采用了以对象为中心的状态表示。一般形式是每个对象都表示为从观察中提取的连续值特征向量。我们将这种表示扩展到对象之间的 n 元关系，其中每个关系都有其对应的特征，类似于场景图特征表示。我们将对象和它们的关系称为实体，它们的特征称为实体特征，例如这种分解反映了每个目标原子 $g_i$ 表示场景中实体的期望符号状态。我们假设环境观察已经在这种以实体为中心的表示中，或者存在一个感知函数 $F$ ，它将观察 $o$ 映射到一组实体特征 $e^i_t \in \mathbb{R}^D,i \in \{ 1, \cdots, N \}$ ，其中 $N$ 是环境中实体的数量， $D$ 是特征维度。

### 5.1 Learning Subgoal Serialization

我们将子目标序列化作为学习问题。如果 $g_j$ 在尝试 $g_i$ 之前需要完成，我们说子目标 $g_i$ 取决于子目标 $gj$ 。

子目标序列化的过程是通过考虑它们之间的所有依赖关系来找到完成所有子目标的最佳顺序。

如何子目标可以以任何顺序完成，则我们称一组子目标是独立的；如何子目标可以以固定顺序完成，则我们称一组子目标是序列化的。

为了了解如何将子目标序列化作为学习问题，我们注意到一组子目标之间的依赖关系可以看作是一个有向图，其中节点是单个子目标，有向边是依赖关系。一对子目标之间的依赖关系和独立性可以表示为有向边及其不存在。我们将子目标块表示为一个完整的子图，同样将不可序列化的目标表示为一个完整的图。然后，子目标块与子目标或其他子目标块之间的依赖关系可以是子图中的任何节点与另一个子图中的外部节点或任何节点之间的边。

现在，我们可以将子目标序列化问题表述为图预测问题。具体来说，给定一个目标 $g = \{ g_1, g_2, \dots, g_K \}$ 和对应的实体特征 $e^g_t = \{ e^{g_1}_t, e^{g_2}_t, \dots, e^{g_K}_t \}$ ，我们的子目标依赖网络是：
$$
f_{dependency}(e^g_t, g) = \phi_{\theta}(\{ [e^{g_i}_t, e^{g_j}_t, g_i, g_j] \}^K_{i,j=1}) = \{ dep(g_i,g_j) \}^K_{i,j=1}
$$
其中 $dep(g_i,g_j) \in [0,1]$ 是一个分数，指示 $g_i$ 是否依赖于 $g_j$ ，$\phi_{\theta}$ 是一个可学习的网络，$[\cdot, \cdot]$ 是连接。

### 5.2 Learning Precondition Prediction

我们已经讨论了如何找到完成一组子目标的最佳顺序，下一步是找到子目标或子目标块的前提条件，这是向后规划的关键步骤。子目标的先决条件是在尝试手头的子目标之前需要完成的另一个目标。为了将其表述为学习问题，我们注意到子目标及其前提条件可能不共享同一组实体。

我们将前提条件问题表述为节点分类问题，其中每个节点对应于场景中的一对目标谓词和实体。我们考虑三个目标类， True 和 False 对应于目标谓词的逻辑状态，第三个目标类 Null 表示谓词不是前提条件的一部分。

具体来说，给定一个目标或子目标集 $g = \{ g_1, \dots, g_K \}$ 和所有实体特征 $e_t$ ，则前提条件网络为：
$$
f_{precondition}(e_t, g) = \phi_{\psi}(\Delta(e_t, g)) = g^{(-1)}
$$
其中 $g^{(-1)}$ 是 $g$ 的预测前提条件， $\phi_{\psi}$ 是一个可学习网络。注意， $g$ 可能只映射到 $e_t$ 的子集。 $\Delta$ 在连接 $e_t$ 和 $g$ 之前用 Null 类填充缺失的谓词。

### 5.3 Learning Subgoal Satisfaction and Reachability

子目标序列化决定了完成子目标的顺序，但一些子目标可能已经完成。在这里，我们使用可学习的模块来确定是否已经满足子目标。我们将子目标满意度问题表述为单个实体分类问题，因为是否满足子目标不依赖于任何其他实体特征。

类似地，我们使用另一个模块来确定子目标是否可以由当前观察的低级控制器到达。我们注意到低级控制器对子目标的可达性可能取决于其他实体的状态。因此，我们将其表述为以所有实体特征为条件的二元分类问题。

给定一个目标 $g$ 及其子目标 $\{ g_1, \dots, g_K \}$ ，模型可以表示为：
$$
f_{satisfied}(e^{g_i}_t, g_i) = \phi_{\alpha}([e^{g_i},g_i]) = sat(g_i) \quad f_{reachable}(e_t,g) = \phi_{\beta}([e_t, g]) = rec(g)
$$
其中。 $sat(g_i) \in [0,1]$ 表示是否满足子目标 $g_i$ ， $rec(g) \in [0,1]$ 表示在给定当前观察的情况下，目标 $g$ 是否可以由低级控制器到达。

### 5.4 Regression Planning with RPN

在描述了 RPN 的基本组成部分之后，我们引入了一种在推理时调用网络以生成计划的算法。

给定实体特征 $e_t$ 和最终目标 $g$ ，第一步是序列化子目标。 我们首先找到所有子目标都不满足 $f_{satisfied}(·)$ 并构建输入节点 $f_{dependency }(·)$，进而预测有向图结构。然后我们使用 Bron-Kerbosch 算法找到所有完整的子图并在所有子图中构造一个 DAG。最后，我们使用拓扑排序来找到具有最高优先级的子目标块。

算法 1 总结了子目标序列化子程序。

![](.\3.png)

给定一个子目标，我们首先检查它是否可以通过具有 $f_{reachable}(·)$ 的低级控制器到达，如果控制器被认为可达，则使用子目标调用控制器。否则， $f_{precondition}(·)$ 用于查找子目标的前提条件并将其设置为新目标。

整个过程如下图所示。

![](.\2.png)



### 5.5 Supervision and Training

**Supervision from demonstrations** ：我们从硬编码专家生成的任务演示中解析训练标签。任务演示由一系列中间目标 $\{ g^{(0)}, \dots, g^{(T)} \}$ 和相应的环境观察 $\{ o^{(0)}, \dots, o^{(T)} \}$ 组成。此外，我们还假设子目标之间的依赖关系 $\{g_0, \dots, g_N \}$ 的目标以有向图的形式给出。

**Training** ：我们用完全监督训练所有子网络。由于我们的架构的递归性质，通过考虑与规划历史无关的中间目标及其前提条件，可以并行优化长规划序列。

## 6. Experiments

我们的实验旨在（1）说明 RPN 的基本特征，尤其是回归规划和子目标序列化的影响，（2）测试 RPN 是否可以对新任务实例实现零样本泛化，以及（3）测试 RPN 是否可以直接从视觉观察输入中学习。

### 6.1 Grid World

### 6.2 Kitchen 3D

我们在此环境中测试 RPN 中每个组件的全部功能，以及完整的回归规划机制是否可以在不丢弃性能的情况下解决以前看不见的任务实例，同时直接处理高维视觉输入。

## 7 Conclusions

我们提出了回归规划网络，这是一种 learning-to-plan 的方法，它在以高维环境观察输入为条件的抽象符号空间中向后规划。我们表明，RPN 中的每个组件在学习复杂的长期任务和泛化到新任务目标方面发挥着重要作用。

在未来的工作中，我们计划将回归规划机制扩展到更复杂但结构化的规划空间，例如几何规划（例如，将对象姿势作为目标的一部分）和程序。我们还打算从视觉演示数据集（例如教学视频）中学习高级计划，以扩展到现实世界的任务。

